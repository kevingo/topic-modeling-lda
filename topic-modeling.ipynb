{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:14:09.926745Z",
     "start_time": "2019-09-07T15:14:09.913731Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 引入 NLTK stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:14:09.937414Z",
     "start_time": "2019-09-07T15:14:09.930643Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'from', 'subject', 're', 'edu', 'use']\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "print(stop_words)\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:14:09.944731Z",
     "start_time": "2019-09-07T15:14:09.940969Z"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    'Convolutional neural network (CNN) delivers impressive achievements in computer vision and machine learning field. However, CNN incurs high computational complexity, especially for vision quality applications because of large image resolution. In this paper, we propose an iterative architecture-aware pruning algorithm with adaptive magnitude threshold while cooperating with quality-metric measurement simultaneously. We show the performance improvement applied on vision quality applications and provide comprehensive analysis with flexible pruning configuration. With the proposed method, the Multiply-Accumulate (MAC) of state-of-the-art low-light imaging (SID) and super-resolution (EDSR) are reduced by 58% and 37% without quality drop, respectively. The memory bandwidth (BW) requirements of convolutional layer can be also reduced by 20% to 40%.',\n",
    "    'Traditional image signal processing (ISP) pipeline consists of a set of individual image processing components onboard a camera to reconstruct a high-quality sRGB image from the sensor raw data. Due to the hand-crafted nature of the ISP components, traditional ISP pipeline has limited reconstruction quality under challenging scenes. Recently, the convolutional neural networks (CNNs) have demonstrated their competitiveness in solving many individual image processing problems, such as image denoising, demosaicking, white balance and contrast enhancement. However, it remains a question whether a CNN model can address the multiple tasks inside an ISP pipeline simultaneously. We make a good attempt along this line and propose a novel framework, which we call CameraNet, for effective and general ISP pipeline learning. The CameraNet is composed of two CNN modules to account for two sets of relatively uncorrelated subtasks in an ISP pipeline: restoration and enhancement. To train the two-stage CameraNet model, we specify two groundtruths that can be easily created in the common workflow of photography. CameraNet is trained to progressively address the restoration and the enhancement subtasks with its two modules. Experiments show that the proposed CameraNet achieves consistently compelling reconstruction quality on three benchmark datasets and outperforms traditional ISP pipelines.',\n",
    "    'We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 做 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:14:09.954254Z",
     "start_time": "2019-09-07T15:14:09.948334Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:14:09.962357Z",
     "start_time": "2019-09-07T15:14:09.956903Z"
    }
   },
   "outputs": [],
   "source": [
    "## data pre-processing\n",
    "\n",
    "# remove escape\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "# remove new line\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:14:09.971870Z",
     "start_time": "2019-09-07T15:14:09.965104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Convolutional neural network (CNN) delivers impressive achievements in '\n",
      " 'computer vision and machine learning field. However, CNN incurs high '\n",
      " 'computational complexity, especially for vision quality applications because '\n",
      " 'of large image resolution. In this paper, we propose an iterative '\n",
      " 'architecture-aware pruning algorithm with adaptive magnitude threshold while '\n",
      " 'cooperating with quality-metric measurement simultaneously. We show the '\n",
      " 'performance improvement applied on vision quality applications and provide '\n",
      " 'comprehensive analysis with flexible pruning configuration. With the '\n",
      " 'proposed method, the Multiply-Accumulate (MAC) of state-of-the-art low-light '\n",
      " 'imaging (SID) and super-resolution (EDSR) are reduced by 58% and 37% without '\n",
      " 'quality drop, respectively. The memory bandwidth (BW) requirements of '\n",
      " 'convolutional layer can be also reduced by 20% to 40%.']\n"
     ]
    }
   ],
   "source": [
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:14:09.986519Z",
     "start_time": "2019-09-07T15:14:09.974556Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['convolutional',\n",
       "  'neural',\n",
       "  'network',\n",
       "  'cnn',\n",
       "  'delivers',\n",
       "  'impressive',\n",
       "  'achievements',\n",
       "  'in',\n",
       "  'computer',\n",
       "  'vision',\n",
       "  'and',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'field',\n",
       "  'however',\n",
       "  'cnn',\n",
       "  'incurs',\n",
       "  'high',\n",
       "  'computational',\n",
       "  'complexity',\n",
       "  'especially',\n",
       "  'for',\n",
       "  'vision',\n",
       "  'quality',\n",
       "  'applications',\n",
       "  'because',\n",
       "  'of',\n",
       "  'large',\n",
       "  'image',\n",
       "  'resolution',\n",
       "  'in',\n",
       "  'this',\n",
       "  'paper',\n",
       "  'we',\n",
       "  'propose',\n",
       "  'an',\n",
       "  'iterative',\n",
       "  'architecture',\n",
       "  'aware',\n",
       "  'pruning',\n",
       "  'algorithm',\n",
       "  'with',\n",
       "  'adaptive',\n",
       "  'magnitude',\n",
       "  'threshold',\n",
       "  'while',\n",
       "  'cooperating',\n",
       "  'with',\n",
       "  'quality',\n",
       "  'metric',\n",
       "  'measurement',\n",
       "  'simultaneously',\n",
       "  'we',\n",
       "  'show',\n",
       "  'the',\n",
       "  'performance',\n",
       "  'improvement',\n",
       "  'applied',\n",
       "  'on',\n",
       "  'vision',\n",
       "  'quality',\n",
       "  'applications',\n",
       "  'and',\n",
       "  'provide',\n",
       "  'comprehensive',\n",
       "  'analysis',\n",
       "  'with',\n",
       "  'flexible',\n",
       "  'pruning',\n",
       "  'configuration',\n",
       "  'with',\n",
       "  'the',\n",
       "  'proposed',\n",
       "  'method',\n",
       "  'the',\n",
       "  'multiply',\n",
       "  'accumulate',\n",
       "  'mac',\n",
       "  'of',\n",
       "  'state',\n",
       "  'of',\n",
       "  'the',\n",
       "  'art',\n",
       "  'low',\n",
       "  'light',\n",
       "  'imaging',\n",
       "  'sid',\n",
       "  'and',\n",
       "  'super',\n",
       "  'resolution',\n",
       "  'edsr',\n",
       "  'are',\n",
       "  'reduced',\n",
       "  'by',\n",
       "  'and',\n",
       "  'without',\n",
       "  'quality',\n",
       "  'drop',\n",
       "  'respectively',\n",
       "  'the',\n",
       "  'memory',\n",
       "  'bandwidth',\n",
       "  'bw',\n",
       "  'requirements',\n",
       "  'of',\n",
       "  'convolutional',\n",
       "  'layer',\n",
       "  'can',\n",
       "  'be',\n",
       "  'also',\n",
       "  'reduced',\n",
       "  'by',\n",
       "  'to'],\n",
       " ['traditional',\n",
       "  'image',\n",
       "  'signal',\n",
       "  'processing',\n",
       "  'isp',\n",
       "  'pipeline',\n",
       "  'consists',\n",
       "  'of',\n",
       "  'set',\n",
       "  'of',\n",
       "  'individual',\n",
       "  'image',\n",
       "  'processing',\n",
       "  'components',\n",
       "  'onboard',\n",
       "  'camera',\n",
       "  'to',\n",
       "  'reconstruct',\n",
       "  'high',\n",
       "  'quality',\n",
       "  'srgb',\n",
       "  'image',\n",
       "  'from',\n",
       "  'the',\n",
       "  'sensor',\n",
       "  'raw',\n",
       "  'data',\n",
       "  'due',\n",
       "  'to',\n",
       "  'the',\n",
       "  'hand',\n",
       "  'crafted',\n",
       "  'nature',\n",
       "  'of',\n",
       "  'the',\n",
       "  'isp',\n",
       "  'components',\n",
       "  'traditional',\n",
       "  'isp',\n",
       "  'pipeline',\n",
       "  'has',\n",
       "  'limited',\n",
       "  'reconstruction',\n",
       "  'quality',\n",
       "  'under',\n",
       "  'challenging',\n",
       "  'scenes',\n",
       "  'recently',\n",
       "  'the',\n",
       "  'convolutional',\n",
       "  'neural',\n",
       "  'networks',\n",
       "  'cnns',\n",
       "  'have',\n",
       "  'demonstrated',\n",
       "  'their',\n",
       "  'competitiveness',\n",
       "  'in',\n",
       "  'solving',\n",
       "  'many',\n",
       "  'individual',\n",
       "  'image',\n",
       "  'processing',\n",
       "  'problems',\n",
       "  'such',\n",
       "  'as',\n",
       "  'image',\n",
       "  'denoising',\n",
       "  'demosaicking',\n",
       "  'white',\n",
       "  'balance',\n",
       "  'and',\n",
       "  'contrast',\n",
       "  'enhancement',\n",
       "  'however',\n",
       "  'it',\n",
       "  'remains',\n",
       "  'question',\n",
       "  'whether',\n",
       "  'cnn',\n",
       "  'model',\n",
       "  'can',\n",
       "  'address',\n",
       "  'the',\n",
       "  'multiple',\n",
       "  'tasks',\n",
       "  'inside',\n",
       "  'an',\n",
       "  'isp',\n",
       "  'pipeline',\n",
       "  'simultaneously',\n",
       "  'we',\n",
       "  'make',\n",
       "  'good',\n",
       "  'attempt',\n",
       "  'along',\n",
       "  'this',\n",
       "  'line',\n",
       "  'and',\n",
       "  'propose',\n",
       "  'novel',\n",
       "  'framework',\n",
       "  'which',\n",
       "  'we',\n",
       "  'call',\n",
       "  'cameranet',\n",
       "  'for',\n",
       "  'effective',\n",
       "  'and',\n",
       "  'general',\n",
       "  'isp',\n",
       "  'pipeline',\n",
       "  'learning',\n",
       "  'the',\n",
       "  'cameranet',\n",
       "  'is',\n",
       "  'composed',\n",
       "  'of',\n",
       "  'two',\n",
       "  'cnn',\n",
       "  'modules',\n",
       "  'to',\n",
       "  'account',\n",
       "  'for',\n",
       "  'two',\n",
       "  'sets',\n",
       "  'of',\n",
       "  'relatively',\n",
       "  'uncorrelated',\n",
       "  'subtasks',\n",
       "  'in',\n",
       "  'an',\n",
       "  'isp',\n",
       "  'pipeline',\n",
       "  'restoration',\n",
       "  'and',\n",
       "  'enhancement',\n",
       "  'to',\n",
       "  'train',\n",
       "  'the',\n",
       "  'two',\n",
       "  'stage',\n",
       "  'cameranet',\n",
       "  'model',\n",
       "  'we',\n",
       "  'specify',\n",
       "  'two',\n",
       "  'groundtruths',\n",
       "  'that',\n",
       "  'can',\n",
       "  'be',\n",
       "  'easily',\n",
       "  'created',\n",
       "  'in',\n",
       "  'the',\n",
       "  'common',\n",
       "  'workflow',\n",
       "  'of',\n",
       "  'photography',\n",
       "  'cameranet',\n",
       "  'is',\n",
       "  'trained',\n",
       "  'to',\n",
       "  'progressively',\n",
       "  'address',\n",
       "  'the',\n",
       "  'restoration',\n",
       "  'and',\n",
       "  'the',\n",
       "  'enhancement',\n",
       "  'subtasks',\n",
       "  'with',\n",
       "  'its',\n",
       "  'two',\n",
       "  'modules',\n",
       "  'experiments',\n",
       "  'show',\n",
       "  'that',\n",
       "  'the',\n",
       "  'proposed',\n",
       "  'cameranet',\n",
       "  'achieves',\n",
       "  'consistently',\n",
       "  'compelling',\n",
       "  'reconstruction',\n",
       "  'quality',\n",
       "  'on',\n",
       "  'three',\n",
       "  'benchmark',\n",
       "  'datasets',\n",
       "  'and',\n",
       "  'outperforms',\n",
       "  'traditional',\n",
       "  'isp',\n",
       "  'pipelines'],\n",
       " ['we',\n",
       "  'present',\n",
       "  'vilbert',\n",
       "  'short',\n",
       "  'for',\n",
       "  'vision',\n",
       "  'and',\n",
       "  'language',\n",
       "  'bert',\n",
       "  'model',\n",
       "  'for',\n",
       "  'learning',\n",
       "  'task',\n",
       "  'agnostic',\n",
       "  'joint',\n",
       "  'representations',\n",
       "  'of',\n",
       "  'image',\n",
       "  'content',\n",
       "  'and',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'we',\n",
       "  'extend',\n",
       "  'the',\n",
       "  'popular',\n",
       "  'bert',\n",
       "  'architecture',\n",
       "  'to',\n",
       "  'multi',\n",
       "  'modal',\n",
       "  'two',\n",
       "  'stream',\n",
       "  'model',\n",
       "  'pro',\n",
       "  'cessing',\n",
       "  'both',\n",
       "  'visual',\n",
       "  'and',\n",
       "  'textual',\n",
       "  'inputs',\n",
       "  'in',\n",
       "  'separate',\n",
       "  'streams',\n",
       "  'that',\n",
       "  'interact',\n",
       "  'through',\n",
       "  'co',\n",
       "  'attentional',\n",
       "  'transformer',\n",
       "  'layers',\n",
       "  'we',\n",
       "  'pretrain',\n",
       "  'our',\n",
       "  'model',\n",
       "  'through',\n",
       "  'two',\n",
       "  'proxy',\n",
       "  'tasks',\n",
       "  'on',\n",
       "  'the',\n",
       "  'large',\n",
       "  'automatically',\n",
       "  'collected',\n",
       "  'conceptual',\n",
       "  'captions',\n",
       "  'dataset',\n",
       "  'and',\n",
       "  'then',\n",
       "  'transfer',\n",
       "  'it',\n",
       "  'to',\n",
       "  'multiple',\n",
       "  'established',\n",
       "  'vision',\n",
       "  'and',\n",
       "  'language',\n",
       "  'tasks',\n",
       "  'visual',\n",
       "  'question',\n",
       "  'answering',\n",
       "  'visual',\n",
       "  'commonsense',\n",
       "  'reasoning',\n",
       "  'referring',\n",
       "  'expressions',\n",
       "  'and',\n",
       "  'caption',\n",
       "  'based',\n",
       "  'image',\n",
       "  'retrieval',\n",
       "  'by',\n",
       "  'making',\n",
       "  'only',\n",
       "  'minor',\n",
       "  'additions',\n",
       "  'to',\n",
       "  'the',\n",
       "  'base',\n",
       "  'architecture',\n",
       "  'we',\n",
       "  'observe',\n",
       "  'significant',\n",
       "  'improvements',\n",
       "  'across',\n",
       "  'tasks',\n",
       "  'compared',\n",
       "  'to',\n",
       "  'existing',\n",
       "  'task',\n",
       "  'specific',\n",
       "  'models',\n",
       "  'achieving',\n",
       "  'state',\n",
       "  'of',\n",
       "  'the',\n",
       "  'art',\n",
       "  'on',\n",
       "  'all',\n",
       "  'four',\n",
       "  'tasks',\n",
       "  'our',\n",
       "  'work',\n",
       "  'represents',\n",
       "  'shift',\n",
       "  'away',\n",
       "  'from',\n",
       "  'learning',\n",
       "  'groundings',\n",
       "  'between',\n",
       "  'vision',\n",
       "  'and',\n",
       "  'language',\n",
       "  'only',\n",
       "  'as',\n",
       "  'part',\n",
       "  'of',\n",
       "  'task',\n",
       "  'training',\n",
       "  'and',\n",
       "  'towards',\n",
       "  'treating',\n",
       "  'visual',\n",
       "  'grounding',\n",
       "  'as',\n",
       "  'pretrainable',\n",
       "  'and',\n",
       "  'transferable',\n",
       "  'capability']]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_words = list(sent_to_words(data))\n",
    "data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:14:10.014662Z",
     "start_time": "2019-09-07T15:14:09.992057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['convolutional_neural', 'network', 'cnn', 'delivers', 'impressive', 'achievements', 'in', 'computer', 'vision_and', 'machine', 'learning', 'field', 'however', 'cnn', 'incurs', 'high', 'computational', 'complexity', 'especially', 'for_vision', 'quality_applications', 'because', 'of', 'large', 'image', 'resolution', 'in', 'this', 'paper', 'we', 'propose', 'an', 'iterative', 'architecture', 'aware', 'pruning', 'algorithm', 'with', 'adaptive', 'magnitude', 'threshold', 'while', 'cooperating', 'with', 'quality', 'metric', 'measurement', 'simultaneously_we', 'show', 'the', 'performance', 'improvement', 'applied', 'on', 'vision_quality', 'applications', 'and', 'provide', 'comprehensive', 'analysis', 'with', 'flexible', 'pruning', 'configuration', 'with', 'the_proposed', 'method', 'the', 'multiply', 'accumulate', 'mac', 'of', 'state_of', 'the_art', 'low', 'light', 'imaging', 'sid', 'and', 'super', 'resolution', 'edsr', 'are', 'reduced_by', 'and', 'without', 'quality', 'drop', 'respectively', 'the', 'memory', 'bandwidth', 'bw', 'requirements', 'of', 'convolutional', 'layer', 'can_be', 'also', 'reduced_by', 'to']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=1, threshold=10) # higher threshold fewer phrases.\n",
    "# trigram = gensim.models.Phrases(bigram[data_words], threshold=10)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "# print(trigram_mod[bigram_mod[data_words[0]]])\n",
    "print(bigram_mod[data_words[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:14:10.032077Z",
     "start_time": "2019-09-07T15:14:10.019238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:14:10.293930Z",
     "start_time": "2019-09-07T15:14:10.035279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['convolutional_neural', 'network', 'cnn', 'deliver', 'impressive', 'achievement', 'computer', 'vision', 'machine', 'learning', 'field', 'however', 'cnn', 'incur', 'high', 'computational', 'complexity', 'especially', 'vision_quality', 'application', 'large', 'image', 'resolution', 'paper', 'propose', 'iterative', 'architecture', 'aware', 'prune', 'algorithm', 'adaptive', 'magnitude', 'threshold', 'cooperate', 'quality', 'metric', 'measurement', 'simultaneously', 'show', 'performance', 'improvement', 'apply', 'vision_quality', 'application', 'provide', 'comprehensive', 'analysis', 'flexible', 'prune', 'configuration', 'propose', 'method', 'multiply', 'accumulate', 'mac', 'state', 'art', 'low', 'light', 'image', 'sid', 'super', 'resolution', 'edsr', 'reduce', 'quality', 'drop', 'respectively', 'memory', 'bandwidth', 'bw', 'requirement', 'convolutional', 'layer', 'also', 'reduce']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA 的輸入\n",
    "\n",
    "使用 gensim 的 LDA 方法時，最重要的兩個輸入是 `corpus` 和 `id2word`。`id2word` 是一個 Dictionary 物件，細節可以參考 [corpora.dictionary – Construct word<->id mappings](https://radimrehurek.com/gensim/corpora/dictionary.html) 的說明。透過 `corpora.Dictionary` 來建立 word 和 id 之間的關係。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:14:56.894702Z",
     "start_time": "2019-09-07T15:14:56.888572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 2), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 2), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 2), (52, 1), (53, 2), (54, 2), (55, 2), (56, 1), (57, 2), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in data_lemmatized]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:14:10.338221Z",
     "start_time": "2019-09-07T15:14:09.939Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:14:10.340612Z",
     "start_time": "2019-09-07T15:14:09.942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:14:10.343290Z",
     "start_time": "2019-09-07T15:14:09.944Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-07T15:14:10.350249Z",
     "start_time": "2019-09-07T15:14:09.946Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
